{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d03aef1-7c18-4b9a-b7af-40a16119018e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:15:06.966561Z",
     "iopub.status.busy": "2024-12-05T12:15:06.965099Z",
     "iopub.status.idle": "2024-12-05T12:15:07.130901Z",
     "shell.execute_reply": "2024-12-05T12:15:07.048227Z",
     "shell.execute_reply.started": "2024-12-05T12:15:06.966380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Feature: tensor([[-0.3328, -0.0308,  0.1009,  0.4461,  0.5876,  0.2351, -0.0224, -0.1435,\n",
      "          0.7598, -0.4877, -0.0706, -0.0286, -0.0939, -0.1340,  0.0523, -0.1344,\n",
      "         -0.2848, -0.1130,  0.1791,  0.3997, -0.0550, -0.0801,  0.4190, -0.1020,\n",
      "          0.2094,  0.0101,  0.5934, -0.4145, -0.1456, -0.2173, -0.3908,  0.2839,\n",
      "          0.1893,  0.3146,  0.1488, -0.0162,  0.8680,  0.6971,  0.5999,  0.1355,\n",
      "          0.2126,  0.6969,  0.0809, -0.4190,  0.2180,  0.1755, -1.2294, -0.1417,\n",
      "          0.1397,  0.3049, -0.5066, -0.2639,  0.0325, -0.0796,  0.4065,  0.3282,\n",
      "          0.2626,  0.5231,  0.0906, -0.4036, -0.2759, -0.1835, -0.1665, -0.1994,\n",
      "          0.0653,  0.4721, -0.0136,  0.8431, -0.0410, -0.1298,  0.2136,  0.0429,\n",
      "          0.4508, -0.2869, -0.1636,  0.4278, -0.5509, -0.2560,  0.2993,  0.2012,\n",
      "         -0.0703, -0.2811,  0.6018,  0.0763,  0.4343, -0.5704, -0.3667, -0.3250,\n",
      "         -0.4640,  0.5785, -0.1030, -0.1797,  0.0746, -0.2202,  0.0262,  0.4571,\n",
      "         -0.2258, -0.0586,  0.1454,  0.1823,  0.4974, -0.5852, -0.2290,  0.2561,\n",
      "         -0.1406, -0.1899, -0.2061,  0.2264, -0.1278, -0.2788, -0.3315, -0.0636,\n",
      "         -0.0116, -0.1106,  0.1939, -0.1488,  0.2717, -0.5285, -0.2943, -0.4966,\n",
      "         -0.3616, -0.4657, -0.3685,  0.1828, -0.6705, -0.3240,  0.1963,  0.7848]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Attention Weights: tensor([[0.1033],\n",
      "        [0.1086],\n",
      "        [0.1046],\n",
      "        [0.0878],\n",
      "        [0.0907],\n",
      "        [0.1163],\n",
      "        [0.0990],\n",
      "        [0.0973],\n",
      "        [0.1032],\n",
      "        [0.0892]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class PatchGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=4, dropout=0.5):\n",
    "        super(PatchGCN, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(GCNConv(in_channels, hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.layers.append(GCNConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.attention_pooling = nn.Linear(out_channels, 1)  # Match out_channels\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.layers):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        attention_weights = torch.softmax(self.attention_pooling(x), dim=0)\n",
    "        global_feature = torch.sum(attention_weights * x, dim=0, keepdim=True)\n",
    "\n",
    "        return global_feature, attention_weights\n",
    "\n",
    "# Graph construction helper function\n",
    "def construct_graph(features, adjacency_matrix):\n",
    "    edge_index = torch.nonzero(adjacency_matrix, as_tuple=False).T\n",
    "    return Data(x=features, edge_index=edge_index)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample node features and adjacency matrix\n",
    "    num_nodes = 10\n",
    "    feature_dim = 1024\n",
    "\n",
    "    features = torch.rand((num_nodes, feature_dim))\n",
    "    adjacency_matrix = torch.eye(num_nodes) + torch.rand((num_nodes, num_nodes)) > 0.5\n",
    "    adjacency_matrix = adjacency_matrix.int()\n",
    "\n",
    "    graph_data = construct_graph(features, adjacency_matrix)\n",
    "\n",
    "    model = PatchGCN(in_channels=feature_dim, hidden_channels=512, out_channels=128, num_layers=4, dropout=0.5)\n",
    "\n",
    "    global_feature, attention_weights = model(graph_data.x, graph_data.edge_index)\n",
    "    print(\"Global Feature:\", global_feature)\n",
    "    print(\"Attention Weights:\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0943e6b5-3af2-4339-bf05-5bfb71765918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
