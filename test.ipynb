{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sklearn.metrics as metrics\n",
    "import sys \n",
    "sys.path.append('src/')\n",
    "from src.MIL.ABMIL import *\n",
    "from src.MIL.VarMIL import *\n",
    "from src.MIL.CLAM import *\n",
    "from src.MIL.TransMIL import *\n",
    "import src.MIL.dsmil as dsmil\n",
    "from src.MIL.ACMIL import *\n",
    "from src.MIL.AttriMIL import *\n",
    "#import src.MIL.DeepGraphConv as dgc\n",
    "from src.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn\n",
    "#!pip install torchvision\n",
    "#!pip install tqdm\n",
    "#!pip install torch_geometric\n",
    "#!pip install matplotlib\n",
    "#!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nmslib\n",
    "#!pip install --no-binary :all: nmslib\n",
    "#import nmslib\n",
    "#!pip install requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timm\n",
    "# from timm.data import resolve_data_config\n",
    "# from timm.data.transforms_factory import create_transform\n",
    "# from huggingface_hub import login\n",
    "\n",
    "# login(token = \"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\")  # login with your User Access Token, found at https://huggingface.co/settings/tokens\n",
    "\n",
    "# # pretrained=True needed to load UNI weights (and download weights for the first time)\n",
    "# # init_values need to be passed in to successfully load LayerScale parameters (e.g. - block.0.ls1.gamma)\n",
    "# model = timm.create_model(\"hf-hub:MahmoodLab/UNI\", pretrained=True, init_values=1e-5, dynamic_img_size=True)\n",
    "# transform = create_transform(**resolve_data_config(model.pretrained_cfg, model=model))\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# image = Image.open(\"UNI/.github/uni.jpg\")\n",
    "# image = transform(image).unsqueeze(dim=0) # Image (torch.Tensor) with shape [1, 3, 224, 224] following image resizing and normalization (ImageNet parameters)\n",
    "# with torch.inference_mode():\n",
    "#     feature_emb = model(image) # Extracted features (torch.Tensor) with shape [1,1024]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ABMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"data/train_dict.pkl\"\n",
    "with open(file_path, 'rb') as f:\n",
    "    train_dict = pickle.load(f)\n",
    "\n",
    "file_path = \"data/test_dict.pkl\"\n",
    "with open(file_path, 'rb') as f:\n",
    "    test_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(190, 196, 1024) (190,)\n"
     ]
    }
   ],
   "source": [
    "X_train = train_dict['embeddings'][:,1:,:]\n",
    "y_train = train_dict['labels']\n",
    "X_test = test_dict['embeddings'][:,1:,:]\n",
    "y_test = test_dict['labels']\n",
    "\n",
    "X_cross_val = np.concatenate((X_train, X_test), axis=0)\n",
    "y_cross_val = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(X_cross_val.shape, y_cross_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to PyTorch tensors\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), \n",
    "                               torch.tensor(y_train, dtype=torch.int))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), \n",
    "                              torch.tensor(y_test, dtype=torch.int))\n",
    "\n",
    "cross_val_dataset = TensorDataset(torch.tensor(X_cross_val, dtype=torch.float32),\n",
    "                                    torch.tensor(y_cross_val, dtype=torch.int))\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 1  # Adjust batch size as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE : Embedding +Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.1510, Train error: 0.0462\n",
      "Epoch: 2, Loss: 0.0808, Train error: 0.0154\n",
      "Epoch: 3, Loss: 0.0691, Train error: 0.0231\n",
      "Epoch: 4, Loss: 0.0796, Train error: 0.0308\n",
      "Epoch: 5, Loss: 0.0808, Train error: 0.0231\n",
      "Epoch: 6, Loss: 0.0569, Train error: 0.0231\n",
      "Epoch: 7, Loss: 0.0563, Train error: 0.0385\n",
      "Epoch: 8, Loss: 0.0273, Train error: 0.0077\n",
      "Epoch: 9, Loss: 0.0460, Train error: 0.0231\n",
      "Epoch: 10, Loss: 0.0773, Train error: 0.0308\n",
      "Epoch: 11, Loss: 0.0700, Train error: 0.0308\n",
      "Epoch: 12, Loss: 0.0646, Train error: 0.0308\n",
      "Epoch: 13, Loss: 0.0620, Train error: 0.0231\n",
      "Epoch: 14, Loss: 0.0677, Train error: 0.0308\n",
      "Epoch: 15, Loss: 0.0621, Train error: 0.0308\n",
      "Epoch: 16, Loss: 0.0644, Train error: 0.0308\n",
      "Epoch: 17, Loss: 0.0551, Train error: 0.0231\n",
      "Epoch: 18, Loss: 0.0670, Train error: 0.0308\n",
      "Epoch: 19, Loss: 0.0691, Train error: 0.0308\n"
     ]
    }
   ],
   "source": [
    "model = Emb_mean()\n",
    "#TRAIN THE MODEL\n",
    "for epoch in range(1, 20):\n",
    "    train(train_loader,epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set, Loss: 1.5770, Test error: 0.2833\n",
      "Accuracy : 0.7166666666666667\n",
      "Precision : 0.6511627906976745\n",
      "Recall : 0.9333333333333333\n",
      "F1 Score : 0.7671232876712328\n"
     ]
    }
   ],
   "source": [
    "test(test_loader,y_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE : Embedding +max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Emb_max()\n",
    "#TRAIN THE MODEL\n",
    "for epoch in range(1, 20):\n",
    "    train(train_loader,epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader,y_test,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.4285, Train error: 0.1833\n",
      "Epoch: 2, Loss: 0.2146, Train error: 0.0833\n",
      "Epoch: 3, Loss: 0.1385, Train error: 0.0667\n",
      "Epoch: 4, Loss: 0.0496, Train error: 0.0167\n",
      "Epoch: 5, Loss: 0.0436, Train error: 0.0333\n",
      "Epoch: 6, Loss: 0.0269, Train error: 0.0000\n",
      "Epoch: 7, Loss: 0.0264, Train error: 0.0167\n",
      "Epoch: 8, Loss: 0.1092, Train error: 0.0500\n",
      "Epoch: 9, Loss: 0.0128, Train error: 0.0167\n",
      "Epoch: 10, Loss: 0.1759, Train error: 0.0000\n",
      "Epoch: 11, Loss: 0.0300, Train error: 0.0000\n",
      "Epoch: 12, Loss: 0.0174, Train error: 0.0000\n",
      "Epoch: 13, Loss: 0.0433, Train error: 0.0167\n",
      "Epoch: 14, Loss: 0.0016, Train error: 0.0167\n",
      "Epoch: 15, Loss: 0.1102, Train error: 0.0167\n",
      "Epoch: 16, Loss: 0.0855, Train error: 0.0333\n",
      "Epoch: 17, Loss: 0.1088, Train error: 0.0500\n",
      "Epoch: 18, Loss: 0.0868, Train error: 0.0333\n",
      "Epoch: 19, Loss: 0.2200, Train error: 0.0500\n"
     ]
    }
   ],
   "source": [
    "model = Attention(hidden_size=512, dropout=0.5)\n",
    "#TRAIN THE MODEL\n",
    "for epoch in range(1, 20):\n",
    "    train(train_loader,epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set, Loss: 1.0598, Test error: 0.2769\n",
      "Accuracy : 0.7230769230769231\n",
      "Precision : 0.6621621621621622\n",
      "Recall : 0.8166666666666667\n",
      "F1 Score : 0.7313432835820896\n"
     ]
    }
   ],
   "source": [
    "test(test_loader,y_train,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch: 1, Loss: 0.6694, Train error: 0.3958\n",
      "Epoch: 2, Loss: 0.3948, Train error: 0.1250\n",
      "Epoch: 3, Loss: 0.1374, Train error: 0.0417\n",
      "Epoch: 4, Loss: 0.0442, Train error: 0.0208\n",
      "Epoch: 5, Loss: 0.0178, Train error: 0.0000\n",
      "Epoch: 6, Loss: 0.0334, Train error: 0.0000\n",
      "Epoch: 7, Loss: 0.1200, Train error: 0.0208\n",
      "Epoch: 8, Loss: 0.0244, Train error: 0.0208\n",
      "Epoch: 9, Loss: 0.1589, Train error: 0.0208\n",
      "Epoch: 10, Loss: 0.1017, Train error: 0.0208\n",
      "Evaluating Fold 1\n",
      "Fold 2/5\n",
      "Epoch: 1, Loss: 0.0310, Train error: 0.0208\n",
      "Epoch: 2, Loss: 0.0553, Train error: 0.0208\n",
      "Epoch: 3, Loss: 0.1496, Train error: 0.0208\n",
      "Epoch: 4, Loss: 0.0070, Train error: 0.0000\n",
      "Epoch: 5, Loss: 0.0073, Train error: 0.0000\n",
      "Epoch: 6, Loss: 0.1976, Train error: 0.0208\n",
      "Epoch: 7, Loss: 0.0633, Train error: 0.0417\n",
      "Epoch: 8, Loss: 0.0205, Train error: 0.0000\n",
      "Epoch: 9, Loss: 0.0088, Train error: 0.0000\n",
      "Epoch: 10, Loss: 0.0818, Train error: 0.0208\n",
      "Evaluating Fold 2\n",
      "Fold 3/5\n",
      "Epoch: 1, Loss: 0.0137, Train error: 0.0000\n",
      "Epoch: 2, Loss: 0.0461, Train error: 0.0417\n",
      "Epoch: 3, Loss: 0.2671, Train error: 0.0208\n",
      "Epoch: 4, Loss: 0.0416, Train error: 0.0208\n",
      "Epoch: 5, Loss: 0.0317, Train error: 0.0417\n",
      "Epoch: 6, Loss: 0.0642, Train error: 0.0000\n",
      "Epoch: 7, Loss: 0.0561, Train error: 0.0417\n",
      "Epoch: 8, Loss: 0.0149, Train error: 0.0208\n",
      "Epoch: 9, Loss: 0.0476, Train error: 0.0000\n",
      "Epoch: 10, Loss: 0.0170, Train error: 0.0000\n",
      "Evaluating Fold 3\n",
      "Fold 4/5\n",
      "Epoch: 1, Loss: 0.0294, Train error: 0.0417\n",
      "Epoch: 2, Loss: 0.0644, Train error: 0.0417\n",
      "Epoch: 3, Loss: 0.0645, Train error: 0.0208\n",
      "Epoch: 4, Loss: 0.0369, Train error: 0.0208\n",
      "Epoch: 5, Loss: 0.0113, Train error: 0.0417\n",
      "Epoch: 6, Loss: 0.0856, Train error: 0.0208\n",
      "Epoch: 7, Loss: 0.1012, Train error: 0.0208\n",
      "Epoch: 8, Loss: 0.0447, Train error: 0.0417\n",
      "Epoch: 9, Loss: 0.0493, Train error: 0.0208\n",
      "Epoch: 10, Loss: 0.0049, Train error: 0.0208\n",
      "Evaluating Fold 4\n",
      "Fold 5/5\n",
      "Epoch: 1, Loss: 0.1660, Train error: 0.0417\n",
      "Epoch: 2, Loss: 0.0036, Train error: 0.0000\n",
      "Epoch: 3, Loss: 0.0516, Train error: 0.0208\n",
      "Epoch: 4, Loss: 0.0482, Train error: 0.0000\n",
      "Epoch: 5, Loss: 0.0212, Train error: 0.0208\n",
      "Epoch: 6, Loss: 0.0059, Train error: 0.0208\n",
      "Epoch: 7, Loss: 0.0137, Train error: 0.0000\n",
      "Epoch: 8, Loss: 0.1105, Train error: 0.0417\n",
      "Epoch: 9, Loss: 0.0091, Train error: 0.0208\n",
      "Epoch: 10, Loss: 0.0039, Train error: 0.0208\n",
      "Evaluating Fold 5\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Test Error: 0.0333\n",
      "Average F1 Score: 0.9596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, np.float64(1.0)),\n",
       " (0.08333333333333333, np.float64(0.9090909090909091)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.08333333333333333, np.float64(0.8888888888888888))]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = Attention(hidden_size=512, dropout=0.5)\n",
    "k_fold_cross_validation(test_dataset, model_class, epochs=10,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_ABMIL = {'hidden_size': [128,512], 'dropout': [0,0.1,0.2,0.3,0.5], 'lr': [0.001, 0.01], 'weight_decay': [0.0005, 0.005]}\n",
    "\n",
    "hyperparam_tuning(train_loader, test_loader, y_test, hyp_ABMIL,Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gated Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GatedAttention(hidden_size=512, dropout=0.1)\n",
    "#TRAIN THE MODEL\n",
    "for epoch in range(1, 20):\n",
    "    train(train_loader,epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_ABMIL = {'hidden_size': [128,512], 'dropout': [0,0.1,0.2,0.3,0.5], 'lr': [0.001, 0.01], 'weight_decay': [0.0005, 0.005]}\n",
    "\n",
    "hyperparam_tuning(train_loader, test_loader, y_test, hyp_ABMIL,GatedAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch: 1, Loss: 0.1755, Train error: 0.1058\n",
      "Epoch: 2, Loss: 0.3067, Train error: 0.0481\n",
      "Epoch: 3, Loss: 0.0374, Train error: 0.0192\n",
      "Epoch: 4, Loss: 0.0770, Train error: 0.0288\n",
      "Epoch: 5, Loss: 0.0655, Train error: 0.0288\n",
      "Epoch: 6, Loss: 0.0858, Train error: 0.0481\n",
      "Epoch: 7, Loss: 0.0291, Train error: 0.0096\n",
      "Epoch: 8, Loss: 0.0864, Train error: 0.0192\n",
      "Epoch: 9, Loss: 0.0252, Train error: 0.0000\n",
      "Epoch: 10, Loss: 0.1089, Train error: 0.0481\n",
      "Evaluating Fold 1\n",
      "Fold 2/5\n",
      "Epoch: 1, Loss: 0.0417, Train error: 0.0192\n",
      "Epoch: 2, Loss: 0.2100, Train error: 0.0577\n",
      "Epoch: 3, Loss: 0.0295, Train error: 0.0096\n",
      "Epoch: 4, Loss: 0.0439, Train error: 0.0192\n",
      "Epoch: 5, Loss: 0.0618, Train error: 0.0288\n",
      "Epoch: 6, Loss: 0.0624, Train error: 0.0096\n",
      "Epoch: 7, Loss: 0.0815, Train error: 0.0192\n",
      "Epoch: 8, Loss: 0.0670, Train error: 0.0192\n",
      "Epoch: 9, Loss: 0.0075, Train error: 0.0000\n",
      "Epoch: 10, Loss: 0.1034, Train error: 0.0096\n",
      "Evaluating Fold 2\n",
      "Fold 3/5\n",
      "Epoch: 1, Loss: 0.0466, Train error: 0.0288\n",
      "Epoch: 2, Loss: 0.0987, Train error: 0.0385\n",
      "Epoch: 3, Loss: 0.0513, Train error: 0.0481\n",
      "Epoch: 4, Loss: 0.1103, Train error: 0.0385\n",
      "Epoch: 5, Loss: 0.1364, Train error: 0.0288\n",
      "Epoch: 6, Loss: 0.0845, Train error: 0.0288\n",
      "Epoch: 7, Loss: 0.1261, Train error: 0.0385\n",
      "Epoch: 8, Loss: 0.0777, Train error: 0.0288\n",
      "Epoch: 9, Loss: 0.0979, Train error: 0.0192\n",
      "Epoch: 10, Loss: 0.0785, Train error: 0.0192\n",
      "Evaluating Fold 3\n",
      "Fold 4/5\n",
      "Epoch: 1, Loss: 0.0998, Train error: 0.0288\n",
      "Epoch: 2, Loss: 0.0434, Train error: 0.0096\n",
      "Epoch: 3, Loss: 0.0768, Train error: 0.0481\n",
      "Epoch: 4, Loss: 0.0389, Train error: 0.0096\n",
      "Epoch: 5, Loss: 0.0676, Train error: 0.0288\n",
      "Epoch: 6, Loss: 0.0783, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.1340, Train error: 0.0288\n",
      "Epoch: 8, Loss: 0.1057, Train error: 0.0385\n",
      "Epoch: 9, Loss: 0.0287, Train error: 0.0096\n",
      "Epoch: 10, Loss: 0.1295, Train error: 0.0385\n",
      "Evaluating Fold 4\n",
      "Fold 5/5\n",
      "Epoch: 1, Loss: 0.1485, Train error: 0.0385\n",
      "Epoch: 2, Loss: 0.0198, Train error: 0.0192\n",
      "Epoch: 3, Loss: 0.0682, Train error: 0.0192\n",
      "Epoch: 4, Loss: 0.0364, Train error: 0.0192\n",
      "Epoch: 5, Loss: 0.0406, Train error: 0.0192\n",
      "Epoch: 6, Loss: 0.0847, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.0863, Train error: 0.0096\n",
      "Epoch: 8, Loss: 0.1326, Train error: 0.0481\n",
      "Epoch: 9, Loss: 0.0828, Train error: 0.0385\n",
      "Epoch: 10, Loss: 0.0579, Train error: 0.0288\n",
      "Evaluating Fold 5\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Test Error: 0.0000\n",
      "Average F1 Score: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0))]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = GatedAttention(hidden_size=512, dropout=0.5)\n",
    "k_fold_cross_validation(test_dataset, model_class, epochs=10,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VARMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VarMIL(embed_size= 1024, hidden_size=500,separate_attn=False, dropout=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(train_loader,epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_VarMIL = {'hidden_size': [128,512],'gated':[True,False], 'separate_attn':[False,True] , 'dropout': [0,0.1,0.2,0.3,0.5], 'n_var_pool':[50,100,150,200],'act_func':['sqrt', 'log', 'sigmoid'], 'lr': [0.001, 0.01], 'weight_decay': [0.0005, 0.005]}\n",
    "\n",
    "hyperparam_tuning(train_loader, test_loader, y_test, hyp_VarMIL,VarMIL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch: 1, Loss: 0.2438, Train error: 0.1250\n",
      "Epoch: 2, Loss: 0.1070, Train error: 0.0577\n",
      "Epoch: 3, Loss: 0.2958, Train error: 0.0673\n",
      "Epoch: 4, Loss: 0.1461, Train error: 0.0385\n",
      "Epoch: 5, Loss: 0.1136, Train error: 0.0769\n",
      "Epoch: 6, Loss: 0.1210, Train error: 0.0385\n",
      "Epoch: 7, Loss: 0.1022, Train error: 0.0288\n",
      "Epoch: 8, Loss: 0.0775, Train error: 0.0385\n",
      "Epoch: 9, Loss: 0.0563, Train error: 0.0192\n",
      "Epoch: 10, Loss: 0.0571, Train error: 0.0192\n",
      "Evaluating Fold 1\n",
      "Fold 2/5\n",
      "Epoch: 1, Loss: 0.0598, Train error: 0.0192\n",
      "Epoch: 2, Loss: 0.0452, Train error: 0.0192\n",
      "Epoch: 3, Loss: 0.0385, Train error: 0.0192\n",
      "Epoch: 4, Loss: 0.0486, Train error: 0.0288\n",
      "Epoch: 5, Loss: 0.0809, Train error: 0.0288\n",
      "Epoch: 6, Loss: 0.0827, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.0611, Train error: 0.0288\n",
      "Epoch: 8, Loss: 0.0430, Train error: 0.0096\n",
      "Epoch: 9, Loss: 0.1491, Train error: 0.0385\n",
      "Epoch: 10, Loss: 0.0542, Train error: 0.0385\n",
      "Evaluating Fold 2\n",
      "Fold 3/5\n",
      "Epoch: 1, Loss: 0.0669, Train error: 0.0192\n",
      "Epoch: 2, Loss: 0.0239, Train error: 0.0192\n",
      "Epoch: 3, Loss: 0.0620, Train error: 0.0385\n",
      "Epoch: 4, Loss: 0.1645, Train error: 0.0385\n",
      "Epoch: 5, Loss: 0.0624, Train error: 0.0192\n",
      "Epoch: 6, Loss: 0.0882, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.0850, Train error: 0.0385\n",
      "Epoch: 8, Loss: 0.0663, Train error: 0.0096\n",
      "Epoch: 9, Loss: 0.1410, Train error: 0.0288\n",
      "Epoch: 10, Loss: 0.0440, Train error: 0.0192\n",
      "Evaluating Fold 3\n",
      "Fold 4/5\n",
      "Epoch: 1, Loss: 0.2319, Train error: 0.0288\n",
      "Epoch: 2, Loss: 0.0791, Train error: 0.0288\n",
      "Epoch: 3, Loss: 0.0703, Train error: 0.0192\n",
      "Epoch: 4, Loss: 0.1606, Train error: 0.0577\n",
      "Epoch: 5, Loss: 0.0491, Train error: 0.0096\n",
      "Epoch: 6, Loss: 0.0688, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.0549, Train error: 0.0288\n",
      "Epoch: 8, Loss: 0.0118, Train error: 0.0000\n",
      "Epoch: 9, Loss: 0.0809, Train error: 0.0288\n",
      "Epoch: 10, Loss: 0.0098, Train error: 0.0000\n",
      "Evaluating Fold 4\n",
      "Fold 5/5\n",
      "Epoch: 1, Loss: 0.0450, Train error: 0.0192\n",
      "Epoch: 2, Loss: 0.0724, Train error: 0.0192\n",
      "Epoch: 3, Loss: 0.1559, Train error: 0.0481\n",
      "Epoch: 4, Loss: 0.1183, Train error: 0.0481\n",
      "Epoch: 5, Loss: 0.0471, Train error: 0.0192\n",
      "Epoch: 6, Loss: 0.0695, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.0829, Train error: 0.0385\n",
      "Epoch: 8, Loss: 0.0634, Train error: 0.0192\n",
      "Epoch: 9, Loss: 0.1857, Train error: 0.0385\n",
      "Epoch: 10, Loss: 0.0622, Train error: 0.0192\n",
      "Evaluating Fold 5\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Test Error: 0.0154\n",
      "Average F1 Score: 0.9800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.07692307692307693, np.float64(0.9)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0))]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = VarMIL(embed_size= 1024, hidden_size=500,separate_attn=False, dropout=0.5)\n",
    "k_fold_cross_validation(test_dataset, model_class, epochs=10,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.2674, Train error: 0.0462\n",
      "Epoch: 2, Loss: 0.0707, Train error: 0.0231\n",
      "Epoch: 3, Loss: 0.0912, Train error: 0.0385\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#TRAIN THE MODEL\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EPFL/MA2/ML/ML4Science/src/utils.py:25\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, model, lr, weight_decay, print_results)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# calculate loss and metrics\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m loss, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_objective\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbag_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m error, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcalculate_classification_error(data, bag_label)\n",
      "File \u001b[0;32m~/EPFL/MA2/ML/ML4Science/src/MIL/CLAM.py:136\u001b[0m, in \u001b[0;36mCLAM_SB.calculate_objective\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    134\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39mlong()\n\u001b[1;32m    135\u001b[0m Y \u001b[38;5;241m=\u001b[39m Y\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \n\u001b[0;32m--> 136\u001b[0m logits, Y_prob, _, A, instance_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43minstance_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m Y_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(Y_prob, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\n\u001b[1;32m    138\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance_loss_fn(logits, Y)\n",
      "File \u001b[0;32m~/EPFL/MA2/ML/ML4Science/src/MIL/CLAM.py:82\u001b[0m, in \u001b[0;36mCLAM_SB.forward\u001b[0;34m(self, h, label, instance_eval, return_features, attention_only)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, h, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, instance_eval\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, attention_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m---> 82\u001b[0m     A,h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# NxK        \u001b[39;00m\n\u001b[1;32m     83\u001b[0m     A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(A, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# KxN\u001b[39;00m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_only:\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/EPFL/MA2/ML/ML4Science/src/MIL/ABMIL.py:72\u001b[0m, in \u001b[0;36mGatedAtt_net.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 72\u001b[0m     A_V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_V\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# KxL\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     A_U \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_U(x)  \u001b[38;5;66;03m# KxL\u001b[39;00m\n\u001b[1;32m     74\u001b[0m     A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_w(A_V \u001b[38;5;241m*\u001b[39m A_U) \u001b[38;5;66;03m# element wise multiplication # KxATTENTION_BRANCHES\u001b[39;00m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/nn/modules/module.py:1741\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 1741\u001b[0m     forward_call \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tracing_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward)\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m             \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CLAM_SB()\n",
    "#TRAIN THE MODEL\n",
    "for epoch in range(1, 20):\n",
    "    train(train_loader,epoch, model, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader,y_test,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_CLAM = {'gated':[True,False],'size_arg':['big','small'], 'dropout': [0,0.1,0.2,0.3,0.5], 'k_samples':[5,8,10,12], 'lr': [0.001, 0.01], 'weight_decay': [0.0005, 0.005]}\n",
    "\n",
    "hyperparam_tuning(train_loader, test_loader, y_test, hyp_CLAM,CLAM_SB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch: 1, Loss: 0.1183, Train error: 0.0385\n",
      "Epoch: 2, Loss: 0.2371, Train error: 0.0577\n",
      "Epoch: 3, Loss: 0.0596, Train error: 0.0192\n",
      "Epoch: 4, Loss: 0.2270, Train error: 0.0385\n",
      "Epoch: 5, Loss: 0.0615, Train error: 0.0481\n",
      "Epoch: 6, Loss: 0.1333, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.1496, Train error: 0.0192\n",
      "Epoch: 8, Loss: 0.7660, Train error: 0.0481\n",
      "Epoch: 9, Loss: 0.2808, Train error: 0.0577\n",
      "Epoch: 10, Loss: 0.2069, Train error: 0.0481\n",
      "Evaluating Fold 1\n",
      "Fold 2/5\n",
      "Epoch: 1, Loss: 0.6512, Train error: 0.0288\n",
      "Epoch: 2, Loss: 0.0999, Train error: 0.0481\n",
      "Epoch: 3, Loss: 0.2615, Train error: 0.0385\n",
      "Epoch: 4, Loss: 0.3594, Train error: 0.0096\n",
      "Epoch: 5, Loss: 0.2926, Train error: 0.0288\n",
      "Epoch: 6, Loss: 0.5524, Train error: 0.0288\n",
      "Epoch: 7, Loss: 0.0114, Train error: 0.0096\n",
      "Epoch: 8, Loss: 0.4323, Train error: 0.0096\n",
      "Epoch: 9, Loss: 0.3126, Train error: 0.0192\n",
      "Epoch: 10, Loss: 0.6282, Train error: 0.0288\n",
      "Evaluating Fold 2\n",
      "Fold 3/5\n",
      "Epoch: 1, Loss: 0.2952, Train error: 0.0096\n",
      "Epoch: 2, Loss: 0.3997, Train error: 0.0192\n",
      "Epoch: 3, Loss: 0.3114, Train error: 0.0096\n",
      "Epoch: 4, Loss: 0.8522, Train error: 0.0865\n",
      "Epoch: 5, Loss: 0.3785, Train error: 0.0192\n",
      "Epoch: 6, Loss: 0.5992, Train error: 0.0192\n",
      "Epoch: 7, Loss: 0.3719, Train error: 0.0385\n",
      "Epoch: 8, Loss: 0.3468, Train error: 0.0192\n",
      "Epoch: 9, Loss: 0.5691, Train error: 0.0192\n",
      "Epoch: 10, Loss: 0.5207, Train error: 0.0288\n",
      "Evaluating Fold 3\n",
      "Fold 4/5\n",
      "Epoch: 1, Loss: 0.1499, Train error: 0.0385\n",
      "Epoch: 2, Loss: 0.3408, Train error: 0.0096\n",
      "Epoch: 3, Loss: 0.3779, Train error: 0.0481\n",
      "Epoch: 4, Loss: 0.4749, Train error: 0.0481\n",
      "Epoch: 5, Loss: 1.0113, Train error: 0.0385\n",
      "Epoch: 6, Loss: 0.3388, Train error: 0.0481\n",
      "Epoch: 7, Loss: 0.1152, Train error: 0.0192\n",
      "Epoch: 8, Loss: 0.7327, Train error: 0.0288\n",
      "Epoch: 9, Loss: 0.5367, Train error: 0.0385\n",
      "Epoch: 10, Loss: 0.5926, Train error: 0.0385\n",
      "Evaluating Fold 4\n",
      "Fold 5/5\n",
      "Epoch: 1, Loss: 0.3558, Train error: 0.0481\n",
      "Epoch: 2, Loss: 0.6604, Train error: 0.0192\n",
      "Epoch: 3, Loss: 0.3339, Train error: 0.0385\n",
      "Epoch: 4, Loss: 0.3157, Train error: 0.0192\n",
      "Epoch: 5, Loss: 0.1389, Train error: 0.0192\n",
      "Epoch: 6, Loss: 0.2428, Train error: 0.0288\n",
      "Epoch: 7, Loss: 0.7348, Train error: 0.0288\n",
      "Epoch: 8, Loss: 0.3653, Train error: 0.0096\n",
      "Epoch: 9, Loss: 0.7786, Train error: 0.0385\n",
      "Epoch: 10, Loss: 0.5212, Train error: 0.0385\n",
      "Evaluating Fold 5\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Test Error: 0.0462\n",
      "Average F1 Score: 0.9459\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.07692307692307693, np.float64(0.9166666666666666)),\n",
       " (0.038461538461538464, np.float64(0.9333333333333333)),\n",
       " (0.038461538461538464, np.float64(0.9565217391304348)),\n",
       " (0.07692307692307693, np.float64(0.9230769230769231)),\n",
       " (0.0, np.float64(1.0))]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_class = CLAM_SB()\n",
    "k_fold_cross_validation(test_dataset, model_class, epochs=10,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nystrom-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransMIL(n_classes=1)\n",
    "for epoch in range(1, 2):\n",
    "    train(train_loader, epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'lr': [0.001, 0.01], 'weight_decay': [0.0005, 0.005], 'n_classes': [1]  }\n",
    "model_class = TransMIL\n",
    "hyperparam_tuning(train_loader, test_loader, y_test, hyperparameters, model_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch: 1, Loss: 0.7696, Train error: 0.3558\n",
      "Epoch: 2, Loss: 0.7292, Train error: 0.3942\n",
      "Epoch: 3, Loss: 0.6929, Train error: 0.1058\n",
      "Epoch: 4, Loss: 0.6372, Train error: 0.0865\n",
      "Epoch: 5, Loss: 0.6692, Train error: 0.2692\n",
      "Epoch: 6, Loss: 0.6872, Train error: 0.3942\n",
      "Epoch: 7, Loss: 0.6116, Train error: 0.1058\n",
      "Epoch: 8, Loss: 0.6109, Train error: 0.0769\n",
      "Epoch: 9, Loss: 0.6195, Train error: 0.1154\n",
      "Epoch: 10, Loss: 0.6165, Train error: 0.1058\n",
      "Evaluating Fold 1\n",
      "Fold 2/5\n",
      "Epoch: 1, Loss: 0.6745, Train error: 0.2212\n",
      "Epoch: 2, Loss: 0.6122, Train error: 0.1635\n",
      "Epoch: 3, Loss: 0.6044, Train error: 0.0673\n",
      "Epoch: 4, Loss: 0.6168, Train error: 0.0769\n",
      "Epoch: 5, Loss: 0.6189, Train error: 0.0865\n",
      "Epoch: 6, Loss: 0.7153, Train error: 0.3077\n",
      "Epoch: 7, Loss: 0.6319, Train error: 0.1154\n",
      "Epoch: 8, Loss: 0.6334, Train error: 0.1635\n",
      "Epoch: 9, Loss: 0.6882, Train error: 0.2115\n",
      "Epoch: 10, Loss: 0.5961, Train error: 0.0769\n",
      "Evaluating Fold 2\n",
      "Fold 3/5\n",
      "Epoch: 1, Loss: 0.5959, Train error: 0.0481\n",
      "Epoch: 2, Loss: 0.5946, Train error: 0.0481\n",
      "Epoch: 3, Loss: 0.6145, Train error: 0.0673\n",
      "Epoch: 4, Loss: 0.5975, Train error: 0.0673\n",
      "Epoch: 5, Loss: 0.6280, Train error: 0.1154\n",
      "Epoch: 6, Loss: 0.5892, Train error: 0.1154\n",
      "Epoch: 7, Loss: 0.6287, Train error: 0.1346\n",
      "Epoch: 8, Loss: 0.5950, Train error: 0.0577\n",
      "Epoch: 9, Loss: 0.6038, Train error: 0.1346\n",
      "Epoch: 10, Loss: 0.5985, Train error: 0.0673\n",
      "Evaluating Fold 3\n",
      "Fold 4/5\n",
      "Epoch: 1, Loss: 0.6500, Train error: 0.0865\n",
      "Epoch: 2, Loss: 0.6039, Train error: 0.1058\n",
      "Epoch: 3, Loss: 0.5995, Train error: 0.0673\n",
      "Epoch: 4, Loss: 0.6043, Train error: 0.0865\n",
      "Epoch: 5, Loss: 0.6136, Train error: 0.1346\n",
      "Epoch: 6, Loss: 0.6232, Train error: 0.0673\n",
      "Epoch: 7, Loss: 0.6024, Train error: 0.0673\n",
      "Epoch: 8, Loss: 0.6416, Train error: 0.0769\n",
      "Epoch: 9, Loss: 0.6154, Train error: 0.0673\n",
      "Epoch: 10, Loss: 0.6173, Train error: 0.0865\n",
      "Evaluating Fold 4\n",
      "Fold 5/5\n",
      "Epoch: 1, Loss: 0.6046, Train error: 0.1058\n",
      "Epoch: 2, Loss: 0.6192, Train error: 0.0481\n",
      "Epoch: 3, Loss: 0.6036, Train error: 0.0865\n",
      "Epoch: 4, Loss: 0.5969, Train error: 0.0962\n",
      "Epoch: 5, Loss: 0.6164, Train error: 0.0865\n",
      "Epoch: 6, Loss: 0.5932, Train error: 0.1058\n",
      "Epoch: 7, Loss: 0.6047, Train error: 0.0481\n",
      "Epoch: 8, Loss: 0.5927, Train error: 0.0769\n",
      "Epoch: 9, Loss: 0.6310, Train error: 0.1058\n",
      "Epoch: 10, Loss: 0.6146, Train error: 0.0962\n",
      "Evaluating Fold 5\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Test Error: 0.0692\n",
      "Average F1 Score: 0.9266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(tensor(0.1538), np.float64(0.8181818181818182)),\n",
       " (tensor(0.), np.float64(1.0)),\n",
       " (tensor(0.1923), np.float64(0.8148148148148148)),\n",
       " (tensor(0.), np.float64(1.0)),\n",
       " (tensor(0.), np.float64(1.0))]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_classifier = dsmil.IClassifier(feature_extractor=nn.Identity(),feature_size=1024, output_class=1)\n",
    "b_classifier = dsmil.BClassifier(input_size=1024, output_class=1, nonlinear=True, passing_v=False)\n",
    "threshold_opti = 0.65\n",
    "model = dsmil.MILNet(i_classifier, b_classifier, threshold_opti)\n",
    "\n",
    "k_fold_cross_validation(train_dataset, model, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set, Loss: 0.7432, Test error: 0.3500\n",
      "Accuracy : 0.65\n",
      "Precision : 0.5957446808510638\n",
      "Recall : 0.9333333333333333\n",
      "F1 Score : 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "test(test_loader, y_test, model, print_results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/432 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 15\u001b[0m\n\u001b[1;32m      1\u001b[0m hyperparameters \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0.4\u001b[39m, \u001b[38;5;241m0.81\u001b[39m, \u001b[38;5;241m0.05\u001b[39m),\n\u001b[1;32m      2\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.3\u001b[39m], \n\u001b[1;32m      3\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_size\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m1024\u001b[39m], \n\u001b[1;32m      8\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_class\u001b[39m\u001b[38;5;124m'\u001b[39m:[\u001b[38;5;241m1\u001b[39m]}\n\u001b[1;32m      9\u001b[0m model_classes \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m: dsmil\u001b[38;5;241m.\u001b[39mIClassifier(feature_extractor\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mIdentity(),feature_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, output_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_classifier\u001b[39m\u001b[38;5;124m'\u001b[39m: dsmil\u001b[38;5;241m.\u001b[39mBClassifier,\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdsmil\u001b[39m\u001b[38;5;124m'\u001b[39m: dsmil\u001b[38;5;241m.\u001b[39mMILNet\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m \u001b[43mhyperparam_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhyperparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/EPFL/MA2/ML/ML4Science/src/utils.py:148\u001b[0m, in \u001b[0;36mhyperparam_tuning\u001b[0;34m(train_loader, test_loader, y_test, hyperparameters, model_class)\u001b[0m\n\u001b[1;32m    146\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m hyperparameter\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m\u001b[38;5;241m.\u001b[39mco_varnames})\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m--> 148\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparameter\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprint_results\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m test_error,f1 \u001b[38;5;241m=\u001b[39m test(test_loader,y_test,model,print_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f1 \u001b[38;5;241m>\u001b[39m best_f1:\n",
      "File \u001b[0;32m~/EPFL/MA2/ML/ML4Science/src/utils.py:17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, epoch, model, lr, weight_decay, print_results)\u001b[0m\n\u001b[1;32m     14\u001b[0m train_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     15\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay) \u001b[38;5;66;03m#betas ?\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m     18\u001b[0m     bag_label \u001b[38;5;241m=\u001b[39m label[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36mTensorDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Anaconda3/envs/ml4science_graph/lib/python3.9/site-packages/torch/utils/data/dataset.py:211\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[43mtensor\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtensors)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hyperparameters = {'threshold': np.arange(0.4, 0.81, 0.05),\n",
    "                   'dropout': [0.1, 0.2, 0.3], \n",
    "                   'lr': [0.001, 0.01], \n",
    "                   'nonlinear':[True,False], \n",
    "                   'passing_v':[True,False], \n",
    "                   'weight_decay': [0.0005, 0.005],\n",
    "                   'input_size':[1024], \n",
    "                   'output_class':[1]}\n",
    "model_classes = {\n",
    "    'i_classifier': dsmil.IClassifier(feature_extractor=nn.Identity(),feature_size=1024, output_class=1),\n",
    "    'b_classifier': dsmil.BClassifier,\n",
    "    'dsmil': dsmil.MILNet\n",
    "}\n",
    "\n",
    "hyperparam_tuning(train_loader, test_loader, y_test, hyperparameters, model_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results fine-tuning**: \n",
    "Best hyperparameters: {'threshold': 0.6499999999999999, 'dropout': 0.2, 'lr': 0.001, 'nonlinear': True, 'passing_v': True, 'weight_decay': 0.0005, 'input_size': 1024, 'output_class': 1} Best error: tensor(0.1000) Best f1: 0.9032258064516129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepGraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-geometric\n",
    "!pip install nmslib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dgc.DeepGraphConv_Surv(n_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(train_loader, epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ACMIL_GA(embed_dim =1024,hidden_size = 512, n_classes =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(train_loader, epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, y_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch: 1, Loss: 0.7614, Train error: 0.3750\n",
      "Epoch: 2, Loss: 0.1533, Train error: 0.0833\n",
      "Epoch: 3, Loss: 0.1275, Train error: 0.0625\n",
      "Epoch: 4, Loss: 0.0509, Train error: 0.0208\n",
      "Epoch: 5, Loss: 0.4604, Train error: 0.0625\n",
      "Epoch: 6, Loss: 0.0791, Train error: 0.0417\n",
      "Epoch: 7, Loss: 0.0044, Train error: 0.0000\n",
      "Epoch: 8, Loss: 0.3047, Train error: 0.1250\n",
      "Epoch: 9, Loss: 0.0091, Train error: 0.0000\n",
      "Epoch: 10, Loss: 0.1297, Train error: 0.0833\n",
      "Evaluating Fold 1\n",
      "Fold 2/5\n",
      "Epoch: 1, Loss: 0.1779, Train error: 0.0208\n",
      "Epoch: 2, Loss: 0.2795, Train error: 0.0833\n",
      "Epoch: 3, Loss: 0.0371, Train error: 0.0208\n",
      "Epoch: 4, Loss: 0.3987, Train error: 0.1250\n",
      "Epoch: 5, Loss: 0.3092, Train error: 0.1250\n",
      "Epoch: 6, Loss: 0.4703, Train error: 0.1250\n",
      "Epoch: 7, Loss: 0.0103, Train error: 0.0000\n",
      "Epoch: 8, Loss: 0.1029, Train error: 0.0208\n",
      "Epoch: 9, Loss: 0.2368, Train error: 0.0625\n",
      "Epoch: 10, Loss: 0.2990, Train error: 0.0833\n",
      "Evaluating Fold 2\n",
      "Fold 3/5\n",
      "Epoch: 1, Loss: 0.0749, Train error: 0.0417\n",
      "Epoch: 2, Loss: 0.0009, Train error: 0.0000\n",
      "Epoch: 3, Loss: 0.2204, Train error: 0.0417\n",
      "Epoch: 4, Loss: 0.0555, Train error: 0.0417\n",
      "Epoch: 5, Loss: 0.0848, Train error: 0.0208\n",
      "Epoch: 6, Loss: 0.0966, Train error: 0.0417\n",
      "Epoch: 7, Loss: 0.1626, Train error: 0.0833\n",
      "Epoch: 8, Loss: 0.0961, Train error: 0.0833\n",
      "Epoch: 9, Loss: 0.3176, Train error: 0.0833\n",
      "Epoch: 10, Loss: 0.1447, Train error: 0.0625\n",
      "Evaluating Fold 3\n",
      "Fold 4/5\n",
      "Epoch: 1, Loss: 0.0071, Train error: 0.0000\n",
      "Epoch: 2, Loss: 0.0387, Train error: 0.0208\n",
      "Epoch: 3, Loss: 0.0089, Train error: 0.0000\n",
      "Epoch: 4, Loss: 0.0788, Train error: 0.0417\n",
      "Epoch: 5, Loss: 0.3049, Train error: 0.1250\n",
      "Epoch: 6, Loss: 0.0373, Train error: 0.0208\n",
      "Epoch: 7, Loss: 0.0780, Train error: 0.0208\n",
      "Epoch: 8, Loss: 0.1137, Train error: 0.0625\n",
      "Epoch: 9, Loss: 0.2484, Train error: 0.1042\n",
      "Epoch: 10, Loss: 0.7047, Train error: 0.1250\n",
      "Evaluating Fold 4\n",
      "Fold 5/5\n",
      "Epoch: 1, Loss: 0.0813, Train error: 0.0208\n",
      "Epoch: 2, Loss: 0.0015, Train error: 0.0000\n",
      "Epoch: 3, Loss: 0.0819, Train error: 0.0208\n",
      "Epoch: 4, Loss: 0.1861, Train error: 0.0417\n",
      "Epoch: 5, Loss: 0.0118, Train error: 0.0000\n",
      "Epoch: 6, Loss: 0.2561, Train error: 0.0417\n",
      "Epoch: 7, Loss: 0.4412, Train error: 0.0625\n",
      "Epoch: 8, Loss: 0.0884, Train error: 0.0417\n",
      "Epoch: 9, Loss: 0.0117, Train error: 0.0000\n",
      "Epoch: 10, Loss: 0.1416, Train error: 0.0417\n",
      "Evaluating Fold 5\n",
      "\n",
      "K-Fold Cross-Validation Results:\n",
      "Average Test Error: 0.0000\n",
      "Average F1 Score: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0)),\n",
       " (0.0, np.float64(1.0))]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ACMIL_GA(embed_dim =1024,hidden_size = 512, n_classes =2)\n",
    "k_fold_cross_validation(train_dataset, model, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AttriMIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttriMIL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, 20):\n",
    "    train(train_loader, epoch, model, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(test_loader, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml4science_graph)",
   "language": "python",
   "name": "ml4science_graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
