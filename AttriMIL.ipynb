{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "719441fc-1bbe-4d0f-892a-45d37d4816a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-05T12:48:48.776614Z",
     "iopub.status.busy": "2024-12-05T12:48:48.775098Z",
     "iopub.status.idle": "2024-12-05T12:48:57.853001Z",
     "shell.execute_reply": "2024-12-05T12:48:57.849911Z",
     "shell.execute_reply.started": "2024-12-05T12:48:48.776521Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Feature: tensor([[ 0.2639,  0.6149, -0.1608, -0.0069, -0.3186, -0.1865,  0.3596, -0.0507,\n",
      "          0.1148, -0.1079,  0.0648,  0.1059, -0.4524, -0.0956,  0.0757,  0.0203,\n",
      "          0.2495, -0.0522,  0.2196,  0.3129, -0.0294, -0.3026,  0.0332,  0.0874,\n",
      "          0.0477,  0.1471,  0.4346, -0.1119, -0.4331, -0.1022,  0.2118,  0.2733,\n",
      "         -0.4519, -0.2439,  0.3235, -0.1835, -0.2859, -0.1376,  0.0564,  0.2608,\n",
      "          0.1397,  0.3809,  0.0424, -0.0729,  0.2603, -0.0425, -0.0673,  0.0048,\n",
      "          0.1821,  0.0872, -0.1333, -0.0859,  0.1721, -0.2886,  0.2003, -0.0436,\n",
      "         -0.0066, -0.0401,  0.0611,  0.3757, -0.0481,  0.2436, -0.2375, -0.4859,\n",
      "          0.1785, -0.3819, -0.2940, -0.0479, -0.2143, -0.2988,  0.1964, -0.1009,\n",
      "          0.2479,  0.1049,  0.5147, -0.0489, -0.1047,  0.0868, -0.3590, -0.1017,\n",
      "         -0.1807, -0.1208, -0.0523, -0.3798, -0.0252, -0.0761, -0.0215, -0.0642,\n",
      "          0.3168,  0.0163, -0.0335, -0.0545,  0.2666,  0.3459, -0.3372, -0.5199,\n",
      "         -0.2841,  0.0130, -0.0172, -0.1315, -0.3809, -0.4206,  0.2619,  0.0882,\n",
      "         -0.0839,  0.1457,  0.0141, -0.1811, -0.0625, -0.0969, -0.3195,  0.0465,\n",
      "         -0.2733, -0.0166,  0.5161,  0.0085,  0.5546,  0.3656,  0.0713, -0.1130,\n",
      "          0.5195,  0.2217, -0.1511, -0.1090, -0.1780,  0.2715, -0.0506, -0.2015]],\n",
      "       grad_fn=<SumBackward1>)\n",
      "Attention Weights: tensor([[0.1263],\n",
      "        [0.0987],\n",
      "        [0.1074],\n",
      "        [0.0855],\n",
      "        [0.0968],\n",
      "        [0.1021],\n",
      "        [0.0811],\n",
      "        [0.0866],\n",
      "        [0.1137],\n",
      "        [0.1017]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class AttriMIL(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, attr_dim, num_layers=4, dropout=0.5):\n",
    "        super(AttriMIL, self).__init__()\n",
    "\n",
    "        self.feature_extraction_layers = nn.ModuleList()\n",
    "        self.feature_extraction_layers.append(GCNConv(in_channels, hidden_channels))\n",
    "\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.feature_extraction_layers.append(GCNConv(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.feature_extraction_layers.append(GCNConv(hidden_channels, out_channels))\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.attribute_attention_layer = nn.Linear(out_channels + attr_dim, 1)  # Combine features and attributes\n",
    "\n",
    "    def forward(self, node_features, edge_index, attributes):\n",
    "        x = node_features\n",
    "        for i, conv in enumerate(self.feature_extraction_layers):\n",
    "            x = conv(x, edge_index)\n",
    "            if i < len(self.feature_extraction_layers) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Concatenate node features with attributes\n",
    "        combined_features = torch.cat([x, attributes], dim=1)\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = torch.softmax(self.attribute_attention_layer(combined_features), dim=0)\n",
    "\n",
    "        # Compute global feature as a weighted sum of node features\n",
    "        global_feature = torch.sum(attention_weights * x, dim=0, keepdim=True)\n",
    "\n",
    "        return global_feature, attention_weights\n",
    "\n",
    "# Graph construction helper function\n",
    "def construct_graph(node_features, adjacency_matrix):\n",
    "    edge_index = torch.nonzero(adjacency_matrix, as_tuple=False).T\n",
    "    return Data(x=node_features, edge_index=edge_index)\n",
    "\n",
    "# Example graph givem\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample node features and adjacency matrix\n",
    "    num_nodes = 10\n",
    "    feature_dim = 1024\n",
    "    attr_dim = 128  # Dimension of attribute vectors\n",
    "\n",
    "    node_features = torch.rand((num_nodes, feature_dim))\n",
    "    attributes = torch.rand((num_nodes, attr_dim))  # Randomly generated attributes\n",
    "    adjacency_matrix = torch.eye(num_nodes) + torch.rand((num_nodes, num_nodes)) > 0.5\n",
    "    adjacency_matrix = adjacency_matrix.int()\n",
    "\n",
    "    graph_data = construct_graph(node_features, adjacency_matrix)\n",
    "\n",
    "    model = AttriMIL(in_channels=feature_dim, hidden_channels=512, out_channels=128, attr_dim=attr_dim, num_layers=4, dropout=0.5)\n",
    "\n",
    "    global_feature, attention_weights = model(graph_data.x, graph_data.edge_index, attributes)\n",
    "    print(\"Global Feature:\", global_feature)\n",
    "    print(\"Attention Weights:\", attention_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c7940-0edf-45cf-883d-607dac53e44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
